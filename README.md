# fin-lakehouse-ml-signals

[![CI](https://github.com/mmestres91/fin-lakehouse-ml-signals/actions/workflows/ci.yml/badge.svg)](https://github.com/mmestres91/fin-lakehouse-ml-signals/actions/workflows/ci.yml)

A reproducible dataâ€‘quality and MLâ€‘feature pipeline for equitiesâ€‘market signals built on:

* **PythonÂ 3.11** (managed by \[Poetry])
* **PolarsÂ â†’Â Pandas** for fast columnar transforms
* **Great Expectationsâ€¯0.18** (Fluent API) for dataâ€‘quality rules
* **Apache AirflowÂ 2.6+** for daily orchestration
* **TerraformÂ â‰¥Â 1.8**Â (+Â AWS providerÂ 5.x) to provision encrypted S3 buckets, logging targets, and KMS keys
* **AWSÂ S3** as storage for raw, curated, and feature data

---

## ðŸš€ Completed work

1. **Raw data ingestion**
   - `spy_yfinance_parquet_ingestion` DAG: pulls SPY prices (via yfinance) and writes Parquet to the **raw S3 bucket**
2. **Data transformation**
   - `transform_market_data_dag`: Polars transforms raw Parquet â†’ curated schema, outputs to **curated S3 bucket**
3. **Data-quality checks**
   - Great Expectations Fluent config in `gx/`
   - `create_expectations.py` + `create_checkpoint.py` helpers
   - `curated_market_dq_dag`: daily @00:00 UTC, runs checkpoint **curated\_market\_ckpt**, syncs HTML DataÂ Docs to S3
   - Current rules: `datetime` exists/not-null/unique; `close` exists/not-null/>0; row count 1â€“10â€¯000
4. **Infrastructure as code**
   - Terraform modules under `infra/` for S3 buckets (raw, curated, features, logs) and KMS keys
   - CI hook (`.github/workflows/precommit.yml`) enforces `terraform fmt` and linting
5. **Local & CI validation**
   - `scripts/validate_curated.py` for CLI/CI check against any Parquet path
   - Pre-commit & GitHub Actions integrate linting, type checks, and DQ validation on PRs
6. **Feature engineering & testing**
   - `features/features_v1.py`: calculates core signals (momentum variants, EMAÂ 9/20, ATR, RSI, MACD, time features)
   - `tests/test_features_v1_duckdb.py`: DuckDB-based PyTest suite to validate feature outputs
   - `poetry run pytest` runs feature tests alongside other CI checks

---

## Repository layout

```text
fin-lakehouse-ml-signals/
â”œâ”€â”€ dags/                         # Airflow TaskFlow DAGs
â”‚   â”œâ”€â”€ spy_yfinance_parquet_ingestion.py  # raw ingest to S3
â”‚   â”œâ”€â”€ transform_market_data_dag.py       # Polars transform â†’ curated Parquet
â”‚   â””â”€â”€ curated_market_dq_dag.py           # Great Expectations validation
â”œâ”€â”€ features/                     # Feature engineering code
â”‚   â””â”€â”€ features_v1.py            # initial feature calculations (momentum, ATR, RSI, MACD, time features)
â”œâ”€â”€ infra/                        # Terraform modules & configs
â”‚   â”œâ”€â”€ main.tf                   # root module (buckets, KMS)
â”‚   â”œâ”€â”€ variables.tf              # shared inputs
â”‚   â”œâ”€â”€ terraform.tfvars.example  # sample values
â”‚   â””â”€â”€ modules/
â”‚       â””â”€â”€ s3_bucket/            # reusable S3 + logging + encryption sub-module
â”œâ”€â”€ gx/                           # Great Expectations project
â”‚   â”œâ”€â”€ great_expectations.yml
â”‚   â”œâ”€â”€ expectations/
â”‚   â””â”€â”€ checkpoints/
â”œâ”€â”€ scripts/                      # one-off helpers & CLI
â”‚   â”œâ”€â”€ create_expectations.py
â”‚   â”œâ”€â”€ create_checkpoint.py
â”‚   â””â”€â”€ validate_curated.py
â”œâ”€â”€ tests/                        # pytest tests (DuckDB & feature checks)
â”‚   â””â”€â”€ test_features_v1_duckdb.py
â”œâ”€â”€ .github/workflows/            # CI pipelines
â”œâ”€â”€ pyproject.toml                # Poetry deps + config
â””â”€â”€ README.md                     # this file
```

---

## Architecture overview

```mermaid
flowchart LR
    %% === Left: Current ===
    subgraph Current["`**Current**`"]
        direction TB
        A[fa:fa-cloud-download-alt<br/>Vendor API]
        B@{ shape: cyl, label: "Raw S3 Bucket" }
        C@{ shape: cyl, label: "Curated S3 Bucket" }
        D@{ shape: doc, label: "Data Docs\ns3://ge-docs" }
    end

    %% === Right: Future ===
    subgraph Future["`**Future**`"]
        direction TB
        E@{ shape: lin-rect, label: "`Feature Build DAG`\n(`build_ml_features`)" }
        F@{ shape: cyl, label: "Feature Store" }
        G@{ shape: lin-rect, label: "`Train ML Models DAG`\n(`train_ml_models`)" }
        H@{ shape: docs, label: "Model Artifacts\nS3 / MLflow" }
        I@{ icon: "fa:rocket", form: "circle", label: "Prediction API\nFastAPI + Lambda" }
        J([BI / Dashboarding])
    end

    %% === Data and Process Flows ===
    A -- "raw_market_ingest_dag" --> B
    B -- "transform_market_data_dag" --> C
    C -- "curated_market_dq_dag\n(Great Expectations)" --> D
    C ==> E
    E -- "Partitioned Parquet\nor Feast/DuckDB" --> F
    F ==> G
    G --> H
    H --> I
    C --> J

    %% === Styling and Links ===
    style Future fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2.5px
    style Current fill:#e3f2fd,stroke:#1565c0,stroke-width:2.5px

    classDef dag fill:#fffde7,stroke:#fbc02d,stroke-width:1.5px
    class E,G dag

    classDef model fill:#f1f8e9,stroke:#388e3c,stroke-width:1.5px
    class H model

    classDef store fill:#ede7f6,stroke:#8e24aa,stroke-width:1.5px
    class F store

    %% Invisible link to keep ordering: Current left, Future right
    Current ~~~ Future

    %% Tooltips (examples)
    click B "Raw S3 landing zone" "This is the raw S3 bucket for initial ingests"
    click C "Curated data S3" "Validated/transformed data"
    click D "GE Docs" "Great Expectations output docs stored in S3"
    click F "Feature Store" "Storage for ML-ready features"
    click H "Model Artifacts" "Trained model versions, stored in S3/MLflow"
    click I "Prediction API" "Real-time predictions via FastAPI & Lambda"
```

---

## Quickâ€‘start (local)

```bash
# clone & install
git clone git@github.com:mmestres91/fin-lakehouse-ml-signals.git
cd fin-lakehouse-ml-signals
poetry install

# provision infra (raw/curated/features/log buckets + KMS)
cd infra
terraform init && terraform apply
cd ..

# build or update GE artifacts
eventuate=2025-07-17
poetry run python scripts/create_expectations.py
poetry run python scripts/create_checkpoint.py

# run validation locally
echo "2025-07-17" | xargs -I{} poetry run python scripts/validate_curated.py --path s3://finlakehouse-curated-mmestres91/market/spy_transformed.parquet --run_date {}

# run feature tests
poetry run pytest
```

If expectations fail the script exits nonâ€‘zero, making it CIâ€‘friendly.

---

## Airflow integration

1. **`raw_market_ingest_dag.py`**
   *Hourly* (configurable) â€“ pulls fresh raw market data from the vendor API and lands it in the **raw S3 bucket** (`s3://finlakehouse-raw-â€¦`).

2. **`transform_market_data_dag.py`**
   *Triggered after ingest completes* â€“ reads raw Parquet, executes Polars transformations, and writes curated output to `s3://finlakehouse-curated-mmestres91/market/`, partitioned by run date.

3. **`curated_market_dq_dag.py`**
   *Daily @Â 00:00â€¯UTC* â€“ validates the latest curated Parquet with Great Expectations and syncs **HTML DataÂ Docs** to `s3://finlakehouse-ge-docs/curated_market/<run_date>/`.

4. **Failure behaviour**
   Any task that encounters schema/API errors or failed expectations marks its DAG run **failed**, cascading to dependent DAGs.

5. **Local dev**

```bash
astro dev start                  # or docker compose up airflow-init
# manually trigger ingest â†’ transform â†’ dq
docker exec <scheduler> airflow dags trigger raw_market_ingest_dag
```

---

## Continuousâ€‘integration hooks

| Stage                      | Check                                                              |
| -------------------------- | ------------------------------------------------------------------ |
| **preâ€‘commit**             | `black`, `flake8`, `terraform_fmt`, etc.                           |
| **GitHubÂ Actions (TBD)**   | `scripts/validate_curated.py` against a sample data file.          |
| **Prod DAG trigger (TBD)** | Workflow that calls the Airflow REST API after a successful merge. |

---

## Current dataâ€‘quality rules

| Column     | Expectation                        |
| ---------- | ---------------------------------- |
| `datetime` | exists, **notâ€‘null**, **unique**   |
| `close`    | exists, notâ€‘null, **>Â 0**          |
| **Table**  | rowâ€‘count betweenÂ `1`Â andÂ `10â€¯000` |

---

## ðŸ”®Â Future Epic: Feature StoreÂ &Â MLÂ Signal Engineering (TBD)

GoalÂ â€“ create reusable features from the curated dataset for ML models.

* **Extract key features**: price momentum, volatility regime, news sentiment (if applicable).
* **Store features** in Parquet (S3) *or* optionally via **Feast** or **DuckDB**.
* **Airflow DAG**: `build_ml_features` will compute & publish features.
* **Versioning strategy** â€“ daily snapshot partitioning (`yyyyâ€‘mmâ€‘dd/`).

---

## ðŸ§ Â Future Epic: MLÂ TrainingÂ +Â Evaluation Pipelines (TBD)

GoalÂ â€“ automate model training on the engineered features.

* **AirflowÂ DAG**: `train_ml_models`

  * Pulls feature data.
  * Trains XGBoost, LSTM, or other models.
  * Logs metrics & stores the model artifact.
* **Model registry** â€“ store artifacts in S3 or **MLflow**.
* **Metric logging** â€“ WeightsÂ &Â Biases integration or plain CSV.

---

## âš¡Â Optional Epics (TBD)

### ðŸ”—Â Integration & API access

* Expose realâ€‘time predictions via FastAPI + AWS Lambda or ECS Fargate.
* IAM/tokenâ€‘based security.

### ðŸ“ŠÂ Dashboarding

* Connect curated S3 data to a BI tool (Superset, Metabase, etc.) for exploratory analytics.

---

## Contributing

1. **Fork & branch** from `main`.
2. Run `preâ€‘commit install` to enable local hooks.
3. Submit a PR â€“ CI lints and DQ validation must pass.

---

## License

MIT License Â©Â 2025 MarkÂ Mestres
